# ç½‘ç»œç¼–ç¨‹ä¸APIè°ƒç”¨è¯¦è§£

## ğŸŒ ç½‘ç»œç¼–ç¨‹åŸºç¡€

### HTTPåè®®åŸºç¡€

HTTPï¼ˆHyperText Transfer Protocolï¼‰æ˜¯ä¸‡ç»´ç½‘çš„åŸºç¡€åè®®ï¼ŒRSSé˜…è¯»å™¨é€šè¿‡HTTPè¯·æ±‚è·å–RSSæ•°æ®ã€‚

#### HTTPè¯·æ±‚æ–¹æ³•
```python
# å¸¸è§çš„HTTPæ–¹æ³•
GET     # è·å–èµ„æºï¼ˆRSSé˜…è¯»å™¨ä¸»è¦ä½¿ç”¨ï¼‰
POST    # æäº¤æ•°æ®
PUT     # æ›´æ–°èµ„æº
DELETE  # åˆ é™¤èµ„æº
```

#### HTTPçŠ¶æ€ç ç†è§£
```python
# æˆåŠŸå“åº”
200     # OK - è¯·æ±‚æˆåŠŸ
201     # Created - èµ„æºåˆ›å»ºæˆåŠŸ

# å®¢æˆ·ç«¯é”™è¯¯
400     # Bad Request - è¯·æ±‚æ ¼å¼é”™è¯¯
401     # Unauthorized - éœ€è¦è®¤è¯
404     # Not Found - èµ„æºä¸å­˜åœ¨

# æœåŠ¡å™¨é”™è¯¯
500     # Internal Server Error - æœåŠ¡å™¨å†…éƒ¨é”™è¯¯
503     # Service Unavailable - æœåŠ¡ä¸å¯ç”¨
```

### requestsåº“è¯¦è§£

#### åŸºæœ¬ç”¨æ³•
```python
import requests

# åŸºæœ¬GETè¯·æ±‚
response = requests.get('https://example.com/rss.xml')

# å¸¦å‚æ•°çš„è¯·æ±‚
params = {'format': 'xml', 'limit': 10}
response = requests.get('https://api.example.com/rss', params=params)

# è®¾ç½®è¯·æ±‚å¤´
headers = {
    'User-Agent': 'RSS Reader 1.0',
    'Accept': 'application/rss+xml, application/xml'
}
response = requests.get(url, headers=headers)
```

#### é¡¹ç›®ä¸­çš„å®é™…åº”ç”¨
è®©æˆ‘ä»¬åˆ†æRSSé˜…è¯»å™¨ä¸­çš„ç½‘ç»œè¯·æ±‚ä»£ç ï¼š

```python
def add_subscription(self, name: str, url: str) -> bool:
    """æ·»åŠ æ–°çš„è®¢é˜…æº"""
    try:
        # éªŒè¯ RSS é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
        print(f"ğŸ” æ­£åœ¨éªŒè¯ RSS é“¾æ¥: {url}")
        response = requests.get(url, timeout=10)  # è®¾ç½®è¶…æ—¶æ—¶é—´
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        
        # å°è¯•è§£æ RSS å†…å®¹
        feed = feedparser.parse(response.content)
        if not feed.entries:
            print("âš ï¸  è¯¥é“¾æ¥ä¼¼ä¹ä¸æ˜¯æœ‰æ•ˆçš„ RSS æºæˆ–æš‚æ— å†…å®¹")
            return False
        
        # ... å…¶ä»–å¤„ç†é€»è¾‘
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return False
```

#### å…³é”®çŸ¥è¯†ç‚¹è§£æ

**1. è¶…æ—¶è®¾ç½® (timeout=10)**
```python
# ä¸ºä»€ä¹ˆéœ€è¦è®¾ç½®è¶…æ—¶ï¼Ÿ
response = requests.get(url, timeout=10)

"""
è¶…æ—¶è®¾ç½®çš„é‡è¦æ€§ï¼š
1. é˜²æ­¢ç¨‹åºæ— é™ç­‰å¾…
2. æå‡ç”¨æˆ·ä½“éªŒ
3. é¿å…ç½‘ç»œé—®é¢˜å¯¼è‡´ç¨‹åºå¡æ­»
4. èµ„æºç®¡ç†ï¼šåŠæ—¶é‡Šæ”¾è¿æ¥
"""
```

**2. é”™è¯¯æ£€æŸ¥ (raise_for_status())**
```python
response.raise_for_status()

"""
è¿™ä¸ªæ–¹æ³•çš„ä½œç”¨ï¼š
- æ£€æŸ¥HTTPçŠ¶æ€ç 
- å¦‚æœçŠ¶æ€ç è¡¨ç¤ºé”™è¯¯ï¼ˆ4xxæˆ–5xxï¼‰ï¼ŒæŠ›å‡ºå¼‚å¸¸
- è®©é”™è¯¯å¤„ç†æ›´åŠ ç»Ÿä¸€å’Œç®€æ´
"""

# ç­‰æ•ˆçš„æ‰‹åŠ¨æ£€æŸ¥
if response.status_code >= 400:
    raise requests.exceptions.HTTPError(f"HTTP {response.status_code}")
```

**3. å¼‚å¸¸å¤„ç†ç­–ç•¥**
```python
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
except requests.exceptions.Timeout:
    print("è¯·æ±‚è¶…æ—¶")
except requests.exceptions.ConnectionError:
    print("è¿æ¥é”™è¯¯")
except requests.exceptions.HTTPError as e:
    print(f"HTTPé”™è¯¯: {e}")
except requests.exceptions.RequestException as e:
    print(f"è¯·æ±‚å¼‚å¸¸: {e}")
```

## ğŸ”„ RSSåè®®ç†è§£

### RSSåè®®åŸºç¡€

RSSï¼ˆReally Simple Syndicationï¼‰æ˜¯ä¸€ç§ç”¨äºå‘å¸ƒç»å¸¸æ›´æ–°å†…å®¹çš„XMLæ ¼å¼ã€‚

#### RSSæ–‡æ¡£ç»“æ„
```xml
<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>ç½‘ç«™æ ‡é¢˜</title>
    <link>ç½‘ç«™é“¾æ¥</link>
    <description>ç½‘ç«™æè¿°</description>
    
    <item>
      <title>æ–‡ç« æ ‡é¢˜</title>
      <link>æ–‡ç« é“¾æ¥</link>
      <description>æ–‡ç« æ‘˜è¦</description>
      <pubDate>å‘å¸ƒæ—¥æœŸ</pubDate>
    </item>
    
    <!-- æ›´å¤šæ–‡ç« ... -->
  </channel>
</rss>
```

### feedparseråº“è¯¦è§£

#### åŸºæœ¬è§£ææµç¨‹
```python
import feedparser

# è§£æRSSå†…å®¹
feed = feedparser.parse(response.content)

# è®¿é—®é¢‘é“ä¿¡æ¯
print(f"é¢‘é“æ ‡é¢˜: {feed.feed.title}")
print(f"é¢‘é“é“¾æ¥: {feed.feed.link}")
print(f"é¢‘é“æè¿°: {feed.feed.description}")

# è®¿é—®æ–‡ç« åˆ—è¡¨
for entry in feed.entries:
    print(f"æ ‡é¢˜: {entry.title}")
    print(f"é“¾æ¥: {entry.link}")
    print(f"æ‘˜è¦: {entry.summary}")
    print(f"å‘å¸ƒæ—¶é—´: {entry.published}")
```

#### é¡¹ç›®ä¸­çš„åº”ç”¨
```python
def fetch_articles(self, url: str, limit: int = 5) -> List[Dict]:
    """è·å–æŒ‡å®š RSS æºçš„æ–‡ç« åˆ—è¡¨"""
    try:
        print(f"ğŸ“¡ æ­£åœ¨è·å–æœ€æ–°æ–‡ç« ...")
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        feed = feedparser.parse(response.content)
        articles = []
        
        for entry in feed.entries[:limit]:  # é™åˆ¶æ–‡ç« æ•°é‡
            article = {
                'title': entry.get('title', 'æ— æ ‡é¢˜'),
                'link': entry.get('link', ''),
                'summary': entry.get('summary', entry.get('description', 'æ— æ‘˜è¦')),
                'published': entry.get('published', 'æœªçŸ¥æ—¥æœŸ')
            }
            articles.append(article)
        
        return articles
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return []
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {e}")
        return []
```

#### æ•°æ®å®‰å…¨å¤„ç†

**1. ä½¿ç”¨ get() æ–¹æ³•é˜²æ­¢KeyError**
```python
# å®‰å…¨çš„æ–¹å¼
title = entry.get('title', 'æ— æ ‡é¢˜')

# å±é™©çš„æ–¹å¼ï¼ˆå¯èƒ½æŠ›å‡ºKeyErrorï¼‰
title = entry['title']

# å¸¦æœ‰å›é€€çš„å®‰å…¨æ–¹å¼
summary = entry.get('summary', entry.get('description', 'æ— æ‘˜è¦'))
```

**2. åˆ—è¡¨åˆ‡ç‰‡é™åˆ¶æ•°æ®é‡**
```python
# é™åˆ¶æ–‡ç« æ•°é‡ï¼Œé˜²æ­¢å†…å­˜å ç”¨è¿‡å¤§
for entry in feed.entries[:limit]:
    # å¤„ç†æ–‡ç« ...
```

## ğŸ›¡ï¸ ç½‘ç»œç¼–ç¨‹æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†ç­–ç•¥

#### åˆ†å±‚é”™è¯¯å¤„ç†
```python
def robust_request(url: str, max_retries: int = 3):
    """å¥å£®çš„ç½‘ç»œè¯·æ±‚å‡½æ•°"""
    for attempt in range(max_retries):
        try:
            response = requests.get(
                url, 
                timeout=10,
                headers={'User-Agent': 'RSS Reader 1.0'}
            )
            response.raise_for_status()
            return response
            
        except requests.exceptions.Timeout:
            print(f"å°è¯• {attempt + 1}: è¯·æ±‚è¶…æ—¶")
            if attempt == max_retries - 1:
                raise
                
        except requests.exceptions.ConnectionError:
            print(f"å°è¯• {attempt + 1}: è¿æ¥å¤±è´¥")
            if attempt == max_retries - 1:
                raise
                
        except requests.exceptions.HTTPError as e:
            print(f"HTTPé”™è¯¯: {e}")
            break  # HTTPé”™è¯¯é€šå¸¸ä¸éœ€è¦é‡è¯•
            
    return None
```

### 2. æ€§èƒ½ä¼˜åŒ–

#### Sessionå¤ç”¨
```python
class RSSReader:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'RSS Reader 1.0'
        })
    
    def fetch_with_session(self, url: str):
        """ä½¿ç”¨Sessionè¿›è¡Œè¯·æ±‚ï¼Œå¤ç”¨è¿æ¥"""
        return self.session.get(url, timeout=10)
```

#### å¹¶å‘è¯·æ±‚ï¼ˆè¿›é˜¶ï¼‰
```python
import asyncio
import aiohttp
from typing import List

async def fetch_multiple_feeds(urls: List[str]) -> List[dict]:
    """å¼‚æ­¥è·å–å¤šä¸ªRSSæº"""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_single_feed(session, url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results

async def fetch_single_feed(session, url: str):
    """å¼‚æ­¥è·å–å•ä¸ªRSSæº"""
    try:
        async with session.get(url, timeout=10) as response:
            content = await response.text()
            return feedparser.parse(content)
    except Exception as e:
        return f"Error fetching {url}: {e}"
```

### 3. ç¼“å­˜ç­–ç•¥

#### ç®€å•ç¼“å­˜å®ç°
```python
import time
from typing import Dict, Tuple

class CachedRSSReader:
    def __init__(self, cache_duration: int = 300):  # 5åˆ†é’Ÿç¼“å­˜
        self.cache: Dict[str, Tuple[List[Dict], float]] = {}
        self.cache_duration = cache_duration
    
    def fetch_articles_cached(self, url: str, limit: int = 5) -> List[Dict]:
        """å¸¦ç¼“å­˜çš„æ–‡ç« è·å–"""
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        if url in self.cache:
            articles, timestamp = self.cache[url]
            if current_time - timestamp < self.cache_duration:
                print("ğŸ“‹ ä½¿ç”¨ç¼“å­˜æ•°æ®")
                return articles[:limit]
        
        # è·å–æ–°æ•°æ®
        articles = self.fetch_articles(url, limit)
        if articles:
            self.cache[url] = (articles, current_time)
        
        return articles
```

## ğŸ” è°ƒè¯•ç½‘ç»œé—®é¢˜

### å¸¸è§é—®é¢˜è¯Šæ–­

#### 1. è¿æ¥é—®é¢˜
```python
def diagnose_connection(url: str):
    """è¯Šæ–­è¿æ¥é—®é¢˜"""
    try:
        response = requests.get(url, timeout=5)
        print(f"âœ… è¿æ¥æˆåŠŸ: {response.status_code}")
        
    except requests.exceptions.Timeout:
        print("âŒ è¿æ¥è¶…æ—¶ - æ£€æŸ¥ç½‘ç»œæˆ–å¢åŠ è¶…æ—¶æ—¶é—´")
        
    except requests.exceptions.ConnectionError:
        print("âŒ è¿æ¥å¤±è´¥ - æ£€æŸ¥URLæˆ–ç½‘ç»œçŠ¶æ€")
        
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTPé”™è¯¯: {e}")
```

#### 2. å†…å®¹éªŒè¯
```python
def validate_rss_content(url: str):
    """éªŒè¯RSSå†…å®¹"""
    try:
        response = requests.get(url, timeout=10)
        content = response.text
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«RSSæ ‡è¯†
        if '<rss' not in content.lower() and '<feed' not in content.lower():
            print("âš ï¸  å†…å®¹å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„RSSæ ¼å¼")
            
        feed = feedparser.parse(content)
        
        if not feed.entries:
            print("âš ï¸  RSSæºä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
        else:
            print(f"âœ… æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")
            
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
```

## ğŸ§ª å®è·µç»ƒä¹ 

### ç»ƒä¹ 1ï¼šå¢å¼ºé”™è¯¯å¤„ç†
```python
def enhanced_add_subscription(self, name: str, url: str) -> bool:
    """å¢å¼ºç‰ˆæ·»åŠ è®¢é˜…æº - ç»ƒä¹ ä»»åŠ¡"""
    # TODO: å®ç°ä»¥ä¸‹åŠŸèƒ½
    # 1. æ·»åŠ é‡è¯•æœºåˆ¶
    # 2. æ”¯æŒæ›´å¤šRSSæ ¼å¼æ£€æµ‹
    # 3. æ·»åŠ è¯¦ç»†çš„é”™è¯¯åˆ†ç±»
    # 4. å®ç°è¿›åº¦æ˜¾ç¤º
    pass
```

### ç»ƒä¹ 2ï¼šå®ç°RSSæ ¼å¼è½¬æ¢
```python
def convert_rss_to_json(self, url: str) -> dict:
    """å°†RSSå†…å®¹è½¬æ¢ä¸ºJSONæ ¼å¼ - ç»ƒä¹ ä»»åŠ¡"""
    # TODO: å®ç°RSSåˆ°JSONçš„è½¬æ¢
    # 1. è·å–RSSå†…å®¹
    # 2. è§£ææ‰€æœ‰å­—æ®µ
    # 3. è½¬æ¢ä¸ºç»“æ„åŒ–JSON
    # 4. æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
    pass
```

### ç»ƒä¹ 3ï¼šæ‰¹é‡è®¢é˜…æºæ£€æŸ¥
```python
def batch_check_subscriptions(self) -> Dict[str, str]:
    """æ‰¹é‡æ£€æŸ¥æ‰€æœ‰è®¢é˜…æºçŠ¶æ€ - ç»ƒä¹ ä»»åŠ¡"""
    # TODO: å®ç°æ‰¹é‡æ£€æŸ¥åŠŸèƒ½
    # 1. éå†æ‰€æœ‰è®¢é˜…æº
    # 2. æ£€æŸ¥æ¯ä¸ªæºçš„çŠ¶æ€
    # 3. ç»Ÿè®¡å¯ç”¨/ä¸å¯ç”¨æ•°é‡
    # 4. è¿”å›è¯¦ç»†æŠ¥å‘Š
    pass
```

## ğŸ“š æ‰©å±•é˜…è¯»

### ç›¸å…³æŠ€æœ¯æ–‡æ¡£
- [requestså®˜æ–¹æ–‡æ¡£](https://docs.python-requests.org/)
- [feedparseræ–‡æ¡£](https://feedparser.readthedocs.io/)
- [RSS 2.0è§„èŒƒ](https://cyber.harvard.edu/rss/rss.html)
- [HTTPåè®®è¯¦è§£](https://developer.mozilla.org/zh-CN/docs/Web/HTTP)

### è¿›é˜¶ä¸»é¢˜
- å¼‚æ­¥ç½‘ç»œç¼–ç¨‹ï¼ˆasyncio + aiohttpï¼‰
- ç½‘ç»œå®‰å…¨å’Œèº«ä»½éªŒè¯
- æ•°æ®å‹ç¼©å’Œä¼ è¾“ä¼˜åŒ–
- ç½‘ç»œç›‘æ§å’Œæ€§èƒ½åˆ†æ

---

> ğŸ’¡ **å­¦ä¹ æç¤º**ï¼šç½‘ç»œç¼–ç¨‹æ˜¯ç°ä»£è½¯ä»¶å¼€å‘çš„é‡è¦æŠ€èƒ½ã€‚é€šè¿‡RSSé˜…è¯»å™¨é¡¹ç›®ï¼Œæ‚¨å¯ä»¥æŒæ¡HTTPè¯·æ±‚ã€é”™è¯¯å¤„ç†ã€æ•°æ®è§£æç­‰æ ¸å¿ƒæ¦‚å¿µã€‚å»ºè®®å¤šå®è·µä¸åŒçš„ç½‘ç»œåœºæ™¯ï¼ŒåŠ æ·±ç†è§£ã€‚

> ğŸš€ **ä¸‹ä¸€æ­¥**ï¼šå­¦ä¹ å®Œç½‘ç»œç¼–ç¨‹åï¼Œå»ºè®®ç»§ç»­é˜…è¯» `06_ç¬¬ä¸‰æ–¹åº“ä½¿ç”¨è¯¦è§£.md`ï¼Œæ·±å…¥äº†è§£é¡¹ç›®ä¸­ä½¿ç”¨çš„å„ç§åº“ã€‚
