"""
RSS parsing and network request handling.
"""

import html
import re
import random
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import feedparser
import requests
from bs4 import BeautifulSoup
from rich.console import Console

from .article_manager import ArticleManager


class RssParser:
    """RSS è§£æå™¨ï¼Œè´Ÿè´£ç½‘ç»œè¯·æ±‚å’Œ RSS æºè§£æ"""
    
    def __init__(self, article_manager: ArticleManager, timeout: int = 10):
        self.timeout = timeout
        self.console = Console()
        self.article_manager = article_manager
    
    def fetch_feed_info(self, url: str) -> Tuple[Optional[str], bool]:
        """
        è·å– RSS æºçš„æ ‡é¢˜ä¿¡æ¯

        Args:
            url (str): RSS æºé“¾æ¥

        Returns:
            Tuple[Optional[str], bool]: (æ ‡é¢˜ï¼Œæ˜¯å¦æˆåŠŸ)
        """
        try:
            print(f"æ­£åœ¨è¯·æ±‚é“¾æ¥ï¼š{url}")
            response = requests.get(url, timeout=self.timeout)
            response.raise_for_status()

            feed = feedparser.parse(response.content)
            
            if feed.bozo:
                print(f" è­¦å‘Šï¼šé“¾æ¥ {url} å¯èƒ½ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ RSS/Atom æºã€‚é”™è¯¯ï¼š{feed.bozo_exception}")

            title = feed.feed.get("title")
            if not title:
                print(" æ— æ³•ä»é“¾æ¥ä¸­è·å–æ ‡é¢˜ï¼Œå°†ä½¿ç”¨é»˜è®¤åç§°ã€‚")
                # æ·»åŠ å¾®ç§’çº§æ—¶é—´æˆ³å’Œéšæœºæ•°ï¼Œç¡®ä¿åç§°å”¯ä¸€æ€§
                timestamp = datetime.now().strftime("%Y%m%d%H%M%S%f")
                random_suffix = random.randint(1000, 9999)
                title = f"æœªå‘½åè®¢é˜…_{timestamp}_{random_suffix}"
            
            print(f"æˆåŠŸè·å–æ ‡é¢˜ï¼š{title}")
            return title, True

        except requests.exceptions.SSLError:
            print(" âŒ SSL è¿æ¥é”™è¯¯ï¼šæ— æ³•å»ºç«‹å®‰å…¨è¿æ¥ï¼Œå¯èƒ½æ˜¯ç½‘ç«™è¯ä¹¦é—®é¢˜")
            return None, False
        except requests.exceptions.Timeout:
            print(" â° è¿æ¥è¶…æ—¶ï¼šç½‘ç»œå“åº”è¿‡æ…¢ï¼Œè¯·ç¨åé‡è¯•")
            return None, False
        except requests.exceptions.ConnectionError:
            print(" ğŸŒ è¿æ¥é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return None, False
        except requests.exceptions.HTTPError as e:
            print(f" ğŸš« HTTP é”™è¯¯ï¼šæœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ç  {e.response.status_code}")
            return None, False
        except requests.exceptions.RequestException:
            print(" ğŸ“¡ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼šè¿æ¥æˆ–ä¼ è¾“è¿‡ç¨‹ä¸­å‘ç”Ÿé—®é¢˜")
            return None, False
        except Exception:
            print(" â“ å¤„ç†è®¢é˜…æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•")
            return None, False
    
    def fetch_articles(self, url: str, count: int = 3) -> List[Dict[str, str]]:
        """
        è·å– RSS æºçš„æœ€æ–°æ–‡ç« ï¼ˆçº¯è·å–åŠŸèƒ½ï¼Œä¸æ¶‰åŠæ•°æ®æŒä¹…åŒ–ï¼‰

        Args:
            url (str): RSS æºé“¾æ¥
            count (int): è·å–æ–‡ç« æ•°é‡

        Returns:
            List[Dict[str, str]]: æ–‡ç« åˆ—è¡¨
        """
        try:
            response = requests.get(url, timeout=self.timeout)
            response.raise_for_status()

            feed = feedparser.parse(response.content)

            if not feed.entries:
                print(f" è­¦å‘Šï¼šé“¾æ¥ {url} æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ã€‚")
                return []

            # è·å–æ–°æ–‡ç« 
            new_articles = []
            for entry in feed.entries[:count]:
                article = {
                    'title': entry.get("title", "æ— æ ‡é¢˜"),
                    'link': entry.get("link", "æ— é“¾æ¥"),
                    'summary': self._clean_html(entry.get("summary", "æ— æ‘˜è¦")),
                    'published': entry.get("published", ""),
                    'fetch_time': datetime.now().isoformat()
                }
                new_articles.append(article)
            
            return new_articles

        except requests.exceptions.SSLError:
            print(" âŒ SSL è¿æ¥é”™è¯¯ï¼šæ— æ³•å»ºç«‹å®‰å…¨è¿æ¥ï¼Œå¯èƒ½æ˜¯ç½‘ç«™è¯ä¹¦é—®é¢˜")
            return []
        except requests.exceptions.Timeout:
            print(" â° è¿æ¥è¶…æ—¶ï¼šç½‘ç»œå“åº”è¿‡æ…¢ï¼Œè¯·ç¨åé‡è¯•")
            return []
        except requests.exceptions.ConnectionError:
            print(" ğŸŒ è¿æ¥é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return []
        except requests.exceptions.HTTPError as e:
            print(f" ğŸš« HTTP é”™è¯¯ï¼šæœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ç  {e.response.status_code}")
            return []
        except requests.exceptions.RequestException as e:
            print(" ğŸ“¡ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼šè¿æ¥æˆ–ä¼ è¾“è¿‡ç¨‹ä¸­å‘ç”Ÿé—®é¢˜")
            return []
        except Exception as e:
            print(" â“ å¤„ç†æ–‡ç« æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•")
            return []
    
    def fetch_and_save_articles(self, url: str, count: int = 3) -> List[Dict[str, str]]:
        """
        è·å– RSS æºçš„æœ€æ–°æ–‡ç« å¹¶ä¿å­˜åˆ° articles_history.json æ–‡ä»¶ä¸­ã€‚

        Args:
            url (str): RSS æºé“¾æ¥
            count (int): è·å–æ–‡ç« æ•°é‡

        Returns:
            List[Dict[str, str]]: æ–‡ç« åˆ—è¡¨
        """
        # è·å–æ–°æ–‡ç« 
        new_articles = self.fetch_articles(url, count)
        
        # å¦‚æœè·å–æˆåŠŸï¼Œåˆ™æ›´æ–°å†å²è®°å½•
        if new_articles:
            self.article_manager.update_articles_history(url, new_articles)
        
        return new_articles
    
    def _clean_html(self, text: str) -> str:
        """ä½¿ç”¨ BeautifulSoup æ¸…ç† HTML æ ‡ç­¾ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹"""
        if not text:
            return text
        
        try:
            # ä½¿ç”¨ BeautifulSoup è§£æ HTML
            soup = BeautifulSoup(text, 'html.parser')
            
            # ç§»é™¤ script å’Œ style æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # è·å–çº¯æ–‡æœ¬
            clean_text = soup.get_text()
            
            # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
            clean_text = re.sub(r'\s+', ' ', clean_text)
            clean_text = html.unescape(clean_text)
            
            return clean_text.strip()
        except Exception:
            # å¦‚æœ BeautifulSoup è§£æå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
            clean_text = re.sub(r'<[^>]+>', '', text)
            clean_text = re.sub(r'\s+', ' ', clean_text)
            clean_text = html.unescape(clean_text)
            return clean_text.strip()
