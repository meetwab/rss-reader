"""
RSS parsing and network request handling.
"""

import html
import re
import random
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import feedparser
import requests
from bs4 import BeautifulSoup
from rich.console import Console

from .article_manager import ArticleManager
from .ai_summarizer_refactored import create_ai_summarizer_from_config


class RssParser:
    """RSS è§£æå™¨ï¼Œè´Ÿè´£ç½‘ç»œè¯·æ±‚å’Œ RSS æºè§£æ"""
    
    def __init__(self, article_manager: ArticleManager, ai_summarizer=None, timeout: int = 10, enable_ai_summary: bool = True):
        self.timeout = timeout
        self.console = Console()
        self.article_manager = article_manager
        self.enable_ai_summary = enable_ai_summary
        
        # ä½¿ç”¨ä¼ å…¥çš„ AI æ‘˜è¦å™¨ï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥åˆ™åˆ›å»ºæ–°çš„
        if ai_summarizer is not None:
            self.ai_summarizer = ai_summarizer
        elif self.enable_ai_summary:
            self.ai_summarizer = create_ai_summarizer_from_config()
        else:
            self.ai_summarizer = None
    
    def fetch_feed_info(self, url: str) -> Tuple[Optional[str], bool]:
        """
        è·å– RSS æºçš„æ ‡é¢˜ä¿¡æ¯

        Args:
            url (str): RSS æºé“¾æ¥

        Returns:
            Tuple[Optional[str], bool]: (æ ‡é¢˜ï¼Œæ˜¯å¦æˆåŠŸ)
        """
        try:
            print(f"æ­£åœ¨è¯·æ±‚é“¾æ¥ï¼š{url}")
            response = requests.get(url, timeout=self.timeout)
            response.raise_for_status()

            feed = feedparser.parse(response.content)
            
            if feed.bozo:
                print(f" è­¦å‘Šï¼šé“¾æ¥ {url} å¯èƒ½ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ RSS/Atom æºã€‚é”™è¯¯ï¼š{feed.bozo_exception}")

            title = getattr(feed.feed, 'title', None) if hasattr(feed, 'feed') else None
            if not title:
                print(" æ— æ³•ä»é“¾æ¥ä¸­è·å–æ ‡é¢˜ï¼Œå°†ä½¿ç”¨é»˜è®¤åç§°ã€‚")
                # æ·»åŠ å¾®ç§’çº§æ—¶é—´æˆ³å’Œéšæœºæ•°ï¼Œç¡®ä¿åç§°å”¯ä¸€æ€§
                timestamp = datetime.now().strftime("%Y%m%d%H%M%S%f")
                random_suffix = random.randint(1000, 9999)
                title = f"æœªå‘½åè®¢é˜…_{timestamp}_{random_suffix}"
            
            print(f"æˆåŠŸè·å–æ ‡é¢˜ï¼š{title}")
            return title, True

        except requests.exceptions.SSLError:
            print(" âŒ SSL è¿æ¥é”™è¯¯ï¼šæ— æ³•å»ºç«‹å®‰å…¨è¿æ¥ï¼Œå¯èƒ½æ˜¯ç½‘ç«™è¯ä¹¦é—®é¢˜")
            return None, False
        except requests.exceptions.Timeout:
            print(" â° è¿æ¥è¶…æ—¶ï¼šç½‘ç»œå“åº”è¿‡æ…¢ï¼Œè¯·ç¨åé‡è¯•")
            return None, False
        except requests.exceptions.ConnectionError:
            print(" ğŸŒ è¿æ¥é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return None, False
        except requests.exceptions.HTTPError as e:
            print(f" ğŸš« HTTP é”™è¯¯ï¼šæœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ç  {e.response.status_code}")
            return None, False
        except requests.exceptions.RequestException:
            print(" ğŸ“¡ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼šè¿æ¥æˆ–ä¼ è¾“è¿‡ç¨‹ä¸­å‘ç”Ÿé—®é¢˜")
            return None, False
        except Exception:
            print(" â“ å¤„ç†è®¢é˜…æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•")
            return None, False
    
    def fetch_articles(self, url: str, count: int = 3) -> List[Dict[str, str]]:
        """
        è·å– RSS æºçš„æœ€æ–°æ–‡ç« ï¼ˆçº¯è·å–åŠŸèƒ½ï¼Œä¸æ¶‰åŠæ•°æ®æŒä¹…åŒ–ï¼‰

        Args:
            url (str): RSS æºé“¾æ¥
            count (int): è·å–æ–‡ç« æ•°é‡

        Returns:
            List[Dict[str, str]]: æ–‡ç« åˆ—è¡¨
        """
        try:
            response = requests.get(url, timeout=self.timeout)
            response.raise_for_status()

            feed = feedparser.parse(response.content)

            if not feed.entries:
                print(f" è­¦å‘Šï¼šé“¾æ¥ {url} æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ã€‚")
                return []

            # è·å–æ–°æ–‡ç« 
            new_articles = []
            for entry in feed.entries[:count]:
                # åŸºç¡€æ–‡ç« ä¿¡æ¯
                title = str(entry.get("title", "æ— æ ‡é¢˜"))
                link = str(entry.get("link", "æ— é“¾æ¥"))
                summary_raw = entry.get("summary", "æ— æ‘˜è¦")
                original_summary = self._clean_html(str(summary_raw) if summary_raw else "æ— æ‘˜è¦")
                published = str(entry.get("published", ""))
                fetch_time = datetime.now().isoformat()
                
                # æ³¨æ„ï¼šAIæ‘˜è¦çš„ç”Ÿæˆå·²ç§»åˆ° _enhance_articles_with_ai æ–¹æ³•ä¸­
                # åœ¨ fetch_and_save_articles æµç¨‹ä¸­è¿›è¡ŒAIå¢å¼ºå¤„ç†
                article = {
                    'title': title,
                    'link': link,
                    'summary': original_summary,
                    'ai_summary': None,  # æ–°æ–‡ç« å…ˆè®¾ä¸ºNoneï¼Œç¨åè¿›è¡ŒAIå¢å¼ºæ—¶ç”Ÿæˆ
                    'published': published,
                    'fetch_time': fetch_time
                }
                new_articles.append(article)
            
            return new_articles

        except requests.exceptions.SSLError:
            print(" âŒ SSL è¿æ¥é”™è¯¯ï¼šæ— æ³•å»ºç«‹å®‰å…¨è¿æ¥ï¼Œå¯èƒ½æ˜¯ç½‘ç«™è¯ä¹¦é—®é¢˜")
            return []
        except requests.exceptions.Timeout:
            print(" â° è¿æ¥è¶…æ—¶ï¼šç½‘ç»œå“åº”è¿‡æ…¢ï¼Œè¯·ç¨åé‡è¯•")
            return []
        except requests.exceptions.ConnectionError:
            print(" ğŸŒ è¿æ¥é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return []
        except requests.exceptions.HTTPError as e:
            print(f" ğŸš« HTTP é”™è¯¯ï¼šæœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ç  {e.response.status_code}")
            return []
        except requests.exceptions.RequestException as e:
            print(" ğŸ“¡ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼šè¿æ¥æˆ–ä¼ è¾“è¿‡ç¨‹ä¸­å‘ç”Ÿé—®é¢˜")
            return []
        except Exception as e:
            print(" â“ å¤„ç†æ–‡ç« æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•")
            return []
    
    def _enhance_articles_with_ai(self, articles: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        ä½¿ç”¨AIå¯¹æ–‡ç« è¿›è¡Œå¢å¼ºå¤„ç†ï¼ˆæ‘˜è¦ç”Ÿæˆç­‰ï¼‰
        
        èŒè´£ï¼š
        1. ä¸ºæ–°æ–‡ç« ç”ŸæˆAIæ‘˜è¦
        2. å¯æ‰©å±•æ”¯æŒå…¶ä»–AIåŠŸèƒ½ï¼ˆå…³é”®è¯æå–ã€åˆ†ç±»ç­‰ï¼‰
        3. ä¿æŒåŸæœ‰æ–‡ç« æ•°æ®ç»“æ„ä¸å˜
        
        Args:
            articles (List[Dict[str, str]]): åŸå§‹æ–‡ç« åˆ—è¡¨
            
        Returns:
            List[Dict[str, str]]: AIå¢å¼ºåçš„æ–‡ç« åˆ—è¡¨
        """
        enhanced_articles = []
        
        for article in articles:
            # ç”ŸæˆAIæ‘˜è¦ï¼ˆå¦‚æœéœ€è¦ä¸”å¯ç”¨ï¼‰
            if not article.get('ai_summary') and self.ai_summarizer and self.ai_summarizer.is_enabled():
                print(f"  æ­£åœ¨ä¸ºæ–‡ç« ã€Œ{article['title']}ã€ç”Ÿæˆ AI æ‘˜è¦...")
                ai_summary = self.ai_summarizer.generate_summary(article['title'], article['summary'])
                if ai_summary:
                    article['ai_summary'] = ai_summary
                    print(f"  âœ… AI æ‘˜è¦ç”ŸæˆæˆåŠŸ")
                else:
                    print(f"  âš ï¸ AI æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹æ‘˜è¦")
            
            # æœªæ¥å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šAIå¢å¼ºåŠŸèƒ½ï¼š
            # article = self._extract_keywords(article)
            # article = self._classify_article(article)
            
            enhanced_articles.append(article)
        
        return enhanced_articles
    
    def fetch_and_save_articles(self, url: str, count: int = 3) -> List[Dict[str, str]]:
        """
        è·å– RSS æºçš„æœ€æ–°æ–‡ç« ï¼Œè¿›è¡ŒAIå¢å¼ºå¤„ç†ï¼Œç„¶åä¿å­˜åˆ°å†å²è®°å½•

        Args:
            url (str): RSS æºé“¾æ¥
            count (int): è·å–æ–‡ç« æ•°é‡

        Returns:
            List[Dict[str, str]]: æ–‡ç« åˆ—è¡¨
        """
        # 1. è·å–åŸå§‹æ–‡ç« 
        new_articles = self.fetch_articles(url, count)
        
        # 2. å¦‚æœè·å–æˆåŠŸï¼Œè¯†åˆ«çœŸæ­£çš„æ–°æ–‡ç« ï¼Œè¿›è¡ŒAIå¢å¼ºï¼Œå†ä¿å­˜
        if new_articles:
            # è¯†åˆ«å“ªäº›æ˜¯çœŸæ­£çš„æ–°æ–‡ç« ï¼ˆé¿å…å¯¹å·²å­˜åœ¨çš„æ–‡ç« é‡å¤å¤„ç†ï¼‰
            truly_new_articles = self._filter_new_articles(url, new_articles)
            
            # åªå¯¹çœŸæ­£çš„æ–°æ–‡ç« è¿›è¡ŒAIå¢å¼ºå¤„ç†
            if truly_new_articles:
                enhanced_new_articles = self._enhance_articles_with_ai(truly_new_articles)
                
                # ä¿å­˜åˆ°å†å²è®°å½•ï¼ˆArticleManageråªè´Ÿè´£å­˜å‚¨ï¼‰
                self.article_manager.update_articles_history(url, enhanced_new_articles)
        
        return new_articles
    
    def _filter_new_articles(self, url: str, articles: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        è¿‡æ»¤å‡ºçœŸæ­£çš„æ–°æ–‡ç« ï¼ˆåŸºäºé“¾æ¥å»é‡ï¼‰
        
        Args:
            url (str): RSSæºé“¾æ¥
            articles (List[Dict[str, str]]): å¾…æ£€æŸ¥çš„æ–‡ç« åˆ—è¡¨
            
        Returns:
            List[Dict[str, str]]: çœŸæ­£çš„æ–°æ–‡ç« åˆ—è¡¨
        """
        # è·å–ç°æœ‰æ–‡ç« çš„é“¾æ¥é›†åˆ
        articles_history = self.article_manager.file_handler.load_articles_history(
            self.article_manager.articles_history_file
        )
        existing_articles = articles_history.get(url, [])
        existing_links = {article['link'] for article in existing_articles}
        
        # åªè¿”å›é“¾æ¥ä¸å­˜åœ¨çš„æ–°æ–‡ç« 
        new_articles = []
        for article in articles:
            if article['link'] not in existing_links:
                new_articles.append(article)
        
        return new_articles
    
    def _clean_html(self, text: str) -> str:
        """ä½¿ç”¨ BeautifulSoup æ¸…ç† HTML æ ‡ç­¾ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹"""
        if not text:
            return text
        
        try:
            # ä½¿ç”¨ BeautifulSoup è§£æ HTML
            soup = BeautifulSoup(text, 'html.parser')
            
            # ç§»é™¤ script å’Œ style æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # è·å–çº¯æ–‡æœ¬
            clean_text = soup.get_text()
            
            # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
            clean_text = re.sub(r'\s+', ' ', clean_text)
            clean_text = html.unescape(clean_text)
            
            return clean_text.strip()
        except Exception:
            # å¦‚æœ BeautifulSoup è§£æå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
            clean_text = re.sub(r'<[^>]+>', '', text)
            clean_text = re.sub(r'\s+', ' ', clean_text)
            clean_text = html.unescape(clean_text)
            return clean_text.strip()
