#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å®é™…RSSæºçš„æ–‡ç« å†å²åŠŸèƒ½
"""

import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from demo_refactored import RssParser

def test_real_rss():
    """æµ‹è¯•çœŸå®çš„RSSæº"""
    print("ğŸ§ª æµ‹è¯•çœŸå®RSSæºçš„æ–‡ç« å†å²åŠŸèƒ½")
    print("="*50)
    
    parser = RssParser()
    
    # ä½¿ç”¨ä¸€ä¸ªå¯é çš„RSSæºè¿›è¡Œæµ‹è¯•
    test_url = "https://www.v2ex.com/index.xml"
    print(f"ğŸ“¡ æµ‹è¯•RSSæº: {test_url}")
    
    # ç¬¬ä¸€æ¬¡è·å–æ–‡ç« 
    print("\n1ï¸âƒ£ ç¬¬ä¸€æ¬¡è·å–æ–‡ç« ...")
    articles1 = parser.fetch_articles(test_url, count=3)
    print(f"âœ… è·å–åˆ° {len(articles1)} ç¯‡æ–‡ç« ")
    
    if articles1:
        print("ğŸ“ æ–‡ç« æ ‡é¢˜:")
        for i, article in enumerate(articles1, 1):
            print(f"  {i}. {article['title'][:50]}...")
    
    # è·å–å†å²è®°å½•
    print("\n2ï¸âƒ£ æ£€æŸ¥å†å²è®°å½•...")
    history1, has_more1, page1, total_pages1 = parser.get_articles_history(test_url, page_size=10, page=1)
    print(f"ğŸ“š å†å²è®°å½•ä¸­æœ‰ {len(history1)} ç¯‡æ–‡ç« ")
    print(f"ğŸ”„ æ˜¯å¦æœ‰æ›´å¤š: {has_more1}")
    print(f"ğŸ“„ å½“å‰é¡µ: {page1}/{total_pages1}")
    
    # æ¨¡æ‹Ÿç¬¬äºŒæ¬¡è·å–ï¼ˆå¯èƒ½æœ‰æ–°æ–‡ç« ï¼‰
    print("\n3ï¸âƒ£ ç¬¬äºŒæ¬¡è·å–æ–‡ç« ...")
    articles2 = parser.fetch_articles(test_url, count=5)
    print(f"âœ… è·å–åˆ° {len(articles2)} ç¯‡æ–‡ç« ")
    
    # å†æ¬¡æ£€æŸ¥å†å²è®°å½•
    print("\n4ï¸âƒ£ æ£€æŸ¥æ›´æ–°åçš„å†å²è®°å½•...")
    history2, has_more2, page2, total_pages2 = parser.get_articles_history(test_url, page_size=10, page=1)
    print(f"ğŸ“š å†å²è®°å½•ä¸­æœ‰ {len(history2)} ç¯‡æ–‡ç« ")
    print(f"ğŸ”„ æ˜¯å¦æœ‰æ›´å¤š: {has_more2}")
    print(f"ğŸ“„ å½“å‰é¡µ: {page2}/{total_pages2}")
    
    # æµ‹è¯•åˆ†é¡µ
    if len(history2) > 3:
        print("\n5ï¸âƒ£ æµ‹è¯•åˆ†é¡µåŠŸèƒ½...")
        page1_articles, has_more_p1, current_p1, total_p1 = parser.get_articles_history(test_url, page_size=3, page=1)
        page2_articles, has_more_p2, current_p2, total_p2 = parser.get_articles_history(test_url, page_size=3, page=2)
        
        print(f"ğŸ“„ ç¬¬1é¡µ: {len(page1_articles)} ç¯‡æ–‡ç« , æœ‰æ›´å¤š: {has_more_p1}, æ€»é¡µæ•°: {total_p1}")
        print(f"ğŸ“„ ç¬¬2é¡µ: {len(page2_articles)} ç¯‡æ–‡ç« , æœ‰æ›´å¤š: {has_more_p2}, æ€»é¡µæ•°: {total_p2}")
    
    # æ˜¾ç¤ºæ–‡ç« æ—¶é—´ä¿¡æ¯
    if history2:
        print("\n6ï¸âƒ£ æœ€æ–°æ–‡ç« çš„æ—¶é—´ä¿¡æ¯:")
        latest_article = history2[0]
        print(f"  æ ‡é¢˜: {latest_article['title'][:40]}...")
        print(f"  å‘å¸ƒæ—¶é—´: {latest_article.get('published', 'N/A')}")
        print(f"  è·å–æ—¶é—´: {latest_article.get('fetch_time', 'N/A')[:19]}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
    
    # è¯¢é—®æ˜¯å¦æ¸…ç†æµ‹è¯•æ•°æ®
    cleanup = input("\næ˜¯å¦æ¸…ç†æµ‹è¯•æ•°æ®? (y/n): ").strip().lower()
    if cleanup == 'y':
        if os.path.exists(parser.articles_history_file):
            os.remove(parser.articles_history_file)
            print(f"ğŸ§¹ å·²æ¸…ç†: {parser.articles_history_file}")

if __name__ == "__main__":
    try:
        test_real_rss()
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
